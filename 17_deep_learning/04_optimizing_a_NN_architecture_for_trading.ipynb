{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Deep NN to predict Asset Price returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we need to explore variations of the design options outlined above because we can rarely be sure from the outset which network architecture best suits the data.\n",
    "\n",
    "In this section, we will explore various options to build a simple feedforward Neural Network to predict asset price returns for a one-day horizon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:38.291434Z",
     "start_time": "2021-02-23T05:45:38.289510Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:39.954191Z",
     "start_time": "2021-02-23T05:45:38.293938Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "from ast import literal_eval as make_tuple\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:39.958084Z",
     "start_time": "2021-02-23T05:45:39.955234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:39.968456Z",
     "start_time": "2021-02-23T05:45:39.959253Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from utils import MultipleTimeSeriesCV, format_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:39.992331Z",
     "start_time": "2021-02-23T05:45:39.969458Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:40.004488Z",
     "start_time": "2021-02-23T05:45:39.993389Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_STORE = '../data/assets.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:40.012549Z",
     "start_time": "2021-02-23T05:45:40.006305Z"
    }
   },
   "outputs": [],
   "source": [
    "results_path = Path('results')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir()\n",
    "    \n",
    "checkpoint_path = results_path / 'logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a stock return series to predict asset price moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop our trading strategy, we use the daily stock returns for some 995 US stocks for the eight year period from 2010 to 2017, and the features developed in Chapter 12 that include volatility and momentum factors as well as lagged returns with cross-sectional and sectoral rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:42.078013Z",
     "start_time": "2021-02-23T05:45:40.014012Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_hdf('../12_gradient_boosting_machines/data.h5', 'model_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 1994931 entries, ('AAPL', Timestamp('2010-01-04 00:00:00')) to ('NTCT', Timestamp('2017-12-29 00:00:00'))\n",
      "Data columns (total 33 columns):\n",
      " #   Column           Non-Null Count    Dtype  \n",
      "---  ------           --------------    -----  \n",
      " 0   dollar_vol_rank  0 non-null        float64\n",
      " 1   rsi              1981001 non-null  float64\n",
      " 2   bb_high          1976026 non-null  float64\n",
      " 3   bb_low           1976022 non-null  float64\n",
      " 4   NATR             1981001 non-null  float64\n",
      " 5   ATR              1981001 non-null  float64\n",
      " 6   PPO              1970056 non-null  float64\n",
      " 7   MACD             1962096 non-null  float64\n",
      " 8   sector           1994931 non-null  int64  \n",
      " 9   r01              1993936 non-null  float64\n",
      " 10  r05              1989956 non-null  float64\n",
      " 11  r10              1984981 non-null  float64\n",
      " 12  r21              1974036 non-null  float64\n",
      " 13  r42              1953141 non-null  float64\n",
      " 14  r63              1932246 non-null  float64\n",
      " 15  r01dec           1993933 non-null  float64\n",
      " 16  r05dec           1989956 non-null  float64\n",
      " 17  r10dec           1984981 non-null  float64\n",
      " 18  r21dec           1974036 non-null  float64\n",
      " 19  r42dec           1953141 non-null  float64\n",
      " 20  r63dec           1932246 non-null  float64\n",
      " 21  r01q_sector      1993933 non-null  float64\n",
      " 22  r05q_sector      1989956 non-null  float64\n",
      " 23  r10q_sector      1984981 non-null  float64\n",
      " 24  r21q_sector      1974036 non-null  float64\n",
      " 25  r42q_sector      1953141 non-null  float64\n",
      " 26  r63q_sector      1932246 non-null  float64\n",
      " 27  r01_fwd          1993936 non-null  float64\n",
      " 28  r05_fwd          1989956 non-null  float64\n",
      " 29  r21_fwd          1974036 non-null  float64\n",
      " 30  year             1994931 non-null  int64  \n",
      " 31  month            1994931 non-null  int64  \n",
      " 32  weekday          1994931 non-null  int64  \n",
      "dtypes: float64(29), int64(4)\n",
      "memory usage: 510.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:42.090238Z",
     "start_time": "2021-02-23T05:45:42.079584Z"
    }
   },
   "outputs": [],
   "source": [
    "outcomes = data.filter(like='fwd').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:42.105424Z",
     "start_time": "2021-02-23T05:45:42.091504Z"
    }
   },
   "outputs": [],
   "source": [
    "lookahead = 1\n",
    "outcome= f'r{lookahead:02}_fwd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:43.061968Z",
     "start_time": "2021-02-23T05:45:42.106337Z"
    }
   },
   "outputs": [],
   "source": [
    "X_cv = data.loc[idx[:, :'2017'], :].drop(outcomes, axis=1)\n",
    "y_cv = data.loc[idx[:, :'2017'], outcome]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:43.160579Z",
     "start_time": "2021-02-23T05:45:43.063097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_cv.index.get_level_values('symbol').unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:43.269555Z",
     "start_time": "2021-02-23T05:45:43.161556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 0 entries\n",
      "Data columns (total 30 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   dollar_vol_rank  0 non-null      float64\n",
      " 1   rsi              0 non-null      float64\n",
      " 2   bb_high          0 non-null      float64\n",
      " 3   bb_low           0 non-null      float64\n",
      " 4   NATR             0 non-null      float64\n",
      " 5   ATR              0 non-null      float64\n",
      " 6   PPO              0 non-null      float64\n",
      " 7   MACD             0 non-null      float64\n",
      " 8   sector           0 non-null      int64  \n",
      " 9   r01              0 non-null      float64\n",
      " 10  r05              0 non-null      float64\n",
      " 11  r10              0 non-null      float64\n",
      " 12  r21              0 non-null      float64\n",
      " 13  r42              0 non-null      float64\n",
      " 14  r63              0 non-null      float64\n",
      " 15  r01dec           0 non-null      float64\n",
      " 16  r05dec           0 non-null      float64\n",
      " 17  r10dec           0 non-null      float64\n",
      " 18  r21dec           0 non-null      float64\n",
      " 19  r42dec           0 non-null      float64\n",
      " 20  r63dec           0 non-null      float64\n",
      " 21  r01q_sector      0 non-null      float64\n",
      " 22  r05q_sector      0 non-null      float64\n",
      " 23  r10q_sector      0 non-null      float64\n",
      " 24  r21q_sector      0 non-null      float64\n",
      " 25  r42q_sector      0 non-null      float64\n",
      " 26  r63q_sector      0 non-null      float64\n",
      " 27  year             0 non-null      int64  \n",
      " 28  month            0 non-null      int64  \n",
      " 29  weekday          0 non-null      int64  \n",
      "dtypes: float64(26), int64(4)\n",
      "memory usage: 781.7+ KB\n"
     ]
    }
   ],
   "source": [
    "X_cv.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate model generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `make_model` function illustrates how to flexibly define various architectural elements for the search process. The dense_layers argument defines both the depth and width of the network as a list of integers. We also use dropout for regularization, expressed as a float in the range [0, 1] to define the probability that a given unit will be excluded from a training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:43.275796Z",
     "start_time": "2021-02-23T05:45:43.271059Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model(dense_layers, activation, dropout):\n",
    "    '''Creates a multi-layer perceptron model\n",
    "    \n",
    "    dense_layers: List of layer sizes; one number per layer\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    for i, layer_size in enumerate(dense_layers, 1):\n",
    "        if i == 1:\n",
    "            model.add(Dense(layer_size, input_dim=X_cv.shape[1]))\n",
    "            model.add(Activation(activation))\n",
    "        else:\n",
    "            model.add(Dense(layer_size))\n",
    "            model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='Adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate multiple configurations with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into a training set for cross-validation, and keep the last 12 months with data as holdout test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:43.289179Z",
     "start_time": "2021-02-23T05:45:43.277271Z"
    }
   },
   "outputs": [],
   "source": [
    "n_splits = 12\n",
    "train_period_length=21 * 12 * 4\n",
    "test_period_length=21 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:43.297459Z",
     "start_time": "2021-02-23T05:45:43.290410Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = MultipleTimeSeriesCV(n_splits=n_splits,\n",
    "                          train_period_length=train_period_length,\n",
    "                          test_period_length=test_period_length,\n",
    "                          lookahead=lookahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CV Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to define our Keras classifier using the make_model function, set cross-validation (see chapter 6 on The Machine Learning Process and following for the OneStepTimeSeriesSplit), and the parameters that we would like to explore. \n",
    "\n",
    "We pick several one- and two-layer configurations, relu and tanh activation functions, and different dropout rates. We could also try out different optimizers (but did not run this experiment to limit what is already a computationally intensive effort):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:43.305908Z",
     "start_time": "2021-02-23T05:45:43.298464Z"
    }
   },
   "outputs": [],
   "source": [
    "dense_layer_opts = [(16, 8), (32, 16), (32, 32), (64, 32)]\n",
    "activation_opts = ['tanh']\n",
    "dropout_opts = [0, .1, .2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:43.314388Z",
     "start_time": "2021-02-23T05:45:43.307047Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = list(product(dense_layer_opts, activation_opts, dropout_opts))\n",
    "np.random.shuffle(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:43.324946Z",
     "start_time": "2021-02-23T05:45:43.315355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To trigger the parameter search, we instantiate a GridSearchCV object, define the fit_params that will be passed to the Keras model’s fit method, and provide the training data to the GridSearchCV fit method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:45:43.332302Z",
     "start_time": "2021-02-23T05:45:43.326769Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_valid_data(X, y, train_idx, test_idx):\n",
    "    x_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]\n",
    "    x_val, y_val = X.iloc[test_idx, :], y.iloc[test_idx]\n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.397856Z",
     "start_time": "2021-02-23T05:45:43.333668Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 32) tanh 0.1 64\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a04299cd19cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_valid_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml4t/17_deep_learning/../utils.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_end\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             train_idx = dates[(dates[self.date_idx] > days[train_start])\n\u001b[0m\u001b[1;32m     53\u001b[0m                               & (dates.date <= days[train_end])].index\n\u001b[1;32m     54\u001b[0m             test_idx = dates[(dates.date > days[test_start])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ic = []\n",
    "scaler = StandardScaler()\n",
    "for params in param_grid:\n",
    "    dense_layers, activation, dropout = params\n",
    "    for batch_size in [64, 256]:\n",
    "        print(dense_layers, activation, dropout, batch_size)\n",
    "        checkpoint_dir = checkpoint_path / str(dense_layers) / activation / str(dropout) / str(batch_size)\n",
    "        if not checkpoint_dir.exists():\n",
    "            checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        start = time()\n",
    "        for fold, (train_idx, test_idx) in enumerate(cv.split(X_cv)):\n",
    "            x_train, y_train, x_val, y_val = get_train_valid_data(X_cv, y_cv, train_idx, test_idx)\n",
    "            x_train = scaler.fit_transform(x_train)\n",
    "            x_val = scaler.transform(x_val)\n",
    "            preds = y_val.to_frame('actual')\n",
    "            r = pd.DataFrame(index=y_val.groupby(level='date').size().index)\n",
    "            model = make_model(dense_layers, activation, dropout)\n",
    "            for epoch in range(20):            \n",
    "                model.fit(x_train,\n",
    "                          y_train,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=1,\n",
    "                          verbose=0,\n",
    "                          shuffle=True,\n",
    "                          validation_data=(x_val, y_val))\n",
    "                model.save_weights((checkpoint_path / f'ckpt_{fold}_{epoch}').as_posix())\n",
    "                preds[epoch] = model.predict(x_val).squeeze()\n",
    "                r[epoch] = preds.groupby(level='date').apply(lambda x: spearmanr(x.actual, x[epoch])[0]).to_frame(epoch)\n",
    "                print(format_time(time()-start), f'{fold + 1:02d} | {epoch + 1:02d} | {r[epoch].mean():7.4f} | {r[epoch].median():7.4f}')\n",
    "            ic.append(r.assign(dense_layers=str(dense_layers), \n",
    "                               activation=activation, \n",
    "                               dropout=dropout,\n",
    "                               batch_size=batch_size,\n",
    "                               fold=fold))       \n",
    "\n",
    "        t = time()-start\n",
    "        pd.concat(ic).to_hdf(results_path / 'scores.h5', 'ic_by_day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate predictive performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.399489Z",
     "start_time": "2021-02-23T05:45:38.349Z"
    }
   },
   "outputs": [],
   "source": [
    "params = ['dense_layers', 'dropout', 'batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.400398Z",
     "start_time": "2021-02-23T05:45:38.351Z"
    }
   },
   "outputs": [],
   "source": [
    "ic = pd.read_hdf(results_path / 'scores.h5', 'ic_by_day').drop('activation', axis=1)\n",
    "ic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.401296Z",
     "start_time": "2021-02-23T05:45:38.353Z"
    }
   },
   "outputs": [],
   "source": [
    "ic.groupby(params).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.402062Z",
     "start_time": "2021-02-23T05:45:38.355Z"
    }
   },
   "outputs": [],
   "source": [
    "ic_long = pd.melt(ic, id_vars=params + ['fold'], var_name='epoch', value_name='ic')\n",
    "ic_long.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.402928Z",
     "start_time": "2021-02-23T05:45:38.357Z"
    }
   },
   "outputs": [],
   "source": [
    "ic_long = ic_long.groupby(params+ ['epoch', 'fold']).ic.mean().to_frame('ic').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.403975Z",
     "start_time": "2021-02-23T05:45:38.360Z"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.relplot(x='epoch', y='ic', col='dense_layers', row='dropout', \n",
    "                data=ic_long[ic_long.dropout>0], kind='line')\n",
    "g.map(plt.axhline, y=0, ls='--', c='k', lw=1)\n",
    "g.savefig(results_path / 'ic_lineplot', dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.404731Z",
     "start_time": "2021-02-23T05:45:38.362Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_ols(ic):\n",
    "    ic.dense_layers = ic.dense_layers.str.replace(', ', '-').str.replace('(', '').str.replace(')', '')\n",
    "    data = pd.melt(ic, id_vars=params, var_name='epoch', value_name='ic')\n",
    "    data.epoch = data.epoch.astype(int).astype(str).apply(lambda x: f'{int(x):02.0f}')\n",
    "    model_data = pd.get_dummies(data.sort_values(params + ['epoch']), columns=['epoch'] + params, drop_first=True).sort_index(1)\n",
    "    model_data.columns = [s.split('_')[-1] for s in model_data.columns]\n",
    "    model = sm.OLS(endog=model_data.ic, exog=sm.add_constant(model_data.drop('ic', axis=1)))\n",
    "    return model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.405558Z",
     "start_time": "2021-02-23T05:45:38.364Z"
    }
   },
   "outputs": [],
   "source": [
    "model = run_ols(ic.drop('fold', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.406263Z",
     "start_time": "2021-02-23T05:45:38.366Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.407080Z",
     "start_time": "2021-02-23T05:45:38.367Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "ci = model.conf_int()\n",
    "errors = ci[1].sub(ci[0]).div(2)\n",
    "\n",
    "coefs = (model.params.to_frame('coef').assign(error=errors)\n",
    "         .reset_index().rename(columns={'index': 'variable'}))\n",
    "coefs = coefs[~coefs['variable'].str.startswith('date') & (coefs.variable != 'const')]\n",
    "\n",
    "coefs.plot(x='variable', y='coef', kind='bar',\n",
    "           ax=ax, color='none', capsize=3,\n",
    "           yerr='error', legend=False, rot=0, title='Impact of Architecture and Training Parameters on Out-of-Sample Performance')\n",
    "ax.set_ylabel('IC')\n",
    "ax.set_xlabel('')\n",
    "ax.scatter(x=pd.np.arange(len(coefs)), marker='_', s=120, y=coefs['coef'], color='black')\n",
    "ax.axhline(y=0, linestyle='--', color='black', linewidth=1)\n",
    "ax.xaxis.set_ticks_position('none')\n",
    "\n",
    "ax.annotate('Batch Size', xy=(.02, -0.1), xytext=(.02, -0.2),\n",
    "            xycoords='axes fraction',\n",
    "            textcoords='axes fraction',\n",
    "            fontsize=11, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white', ec='black'),\n",
    "            arrowprops=dict(arrowstyle='-[, widthB=1.3, lengthB=0.8', lw=1.0, color='black'))\n",
    "\n",
    "ax.annotate('Layers', xy=(.1, -0.1), xytext=(.1, -0.2),\n",
    "            xycoords='axes fraction',\n",
    "            textcoords='axes fraction',\n",
    "            fontsize=11, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white', ec='black'),\n",
    "            arrowprops=dict(arrowstyle='-[, widthB=4.8, lengthB=0.8', lw=1.0, color='black'))\n",
    "\n",
    "ax.annotate('Dropout', xy=(.2, -0.1), xytext=(.2, -0.2),\n",
    "            xycoords='axes fraction',\n",
    "            textcoords='axes fraction',\n",
    "            fontsize=11, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white', ec='black'),\n",
    "            arrowprops=dict(arrowstyle='-[, widthB=2.8, lengthB=0.8', lw=1.0, color='black'))\n",
    "\n",
    "ax.annotate('Epochs', xy=(.62, -0.1), xytext=(.62, -0.2),\n",
    "            xycoords='axes fraction',\n",
    "            textcoords='axes fraction',\n",
    "            fontsize=11, ha='center', va='bottom',\n",
    "            bbox=dict(boxstyle='square', fc='white', ec='black'),\n",
    "            arrowprops=dict(arrowstyle='-[, widthB=30.5, lengthB=1.0', lw=1.0, color='black'))\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_path / 'ols_coef', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.407955Z",
     "start_time": "2021-02-23T05:45:38.370Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_best_params(n=5):\n",
    "    \"\"\"Get the best parameters across all folds by daily median IC\"\"\"\n",
    "    params = ['dense_layers', 'activation', 'dropout', 'batch_size']\n",
    "    ic = pd.read_hdf(results_path / 'scores.h5', 'ic_by_day').drop('fold', axis=1)\n",
    "    dates = sorted(ic.index.unique())\n",
    "    train_period = 24 * 21\n",
    "    train_dates = dates[:train_period]\n",
    "    ic = ic.loc[train_dates]\n",
    "    return (ic.groupby(params)\n",
    "            .median()\n",
    "            .stack()\n",
    "            .to_frame('ic')\n",
    "            .reset_index()\n",
    "            .rename(columns={'level_4': 'epoch'})\n",
    "            .nlargest(n=n, columns='ic')\n",
    "            .drop('ic', axis=1)\n",
    "            .to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.408720Z",
     "start_time": "2021-02-23T05:45:38.373Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_predictions(dense_layers, activation, dropout, batch_size, epoch):\n",
    "    data = pd.read_hdf('../12_gradient_boosting_machines/data.h5', 'model_data').dropna()\n",
    "    outcomes = data.filter(like='fwd').columns.tolist()\n",
    "    X_cv = data.loc[idx[:, :'2017'], :].drop(outcomes, axis=1)\n",
    "    input_dim = X_cv.shape[1]\n",
    "    y_cv = data.loc[idx[:, :'2017'], 'r01_fwd']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    predictions = []\n",
    "    \n",
    "    do = '0' if str(dropout) == '0.0' else str(dropout)\n",
    "    checkpoint_dir = checkpoint_path / dense_layers / activation / do / str(batch_size)\n",
    "        \n",
    "    for fold, (train_idx, test_idx) in enumerate(cv.split(X_cv)):\n",
    "        x_train, y_train, x_val, y_val = get_train_valid_data(X_cv, y_cv, train_idx, test_idx)\n",
    "        x_val = scaler.fit(x_train).transform(x_val)\n",
    "        model = make_model(make_tuple(dense_layers), activation, dropout)\n",
    "        status = model.load_weights((checkpoint_dir / f'ckpt_{fold}_{epoch}').as_posix())\n",
    "        status.expect_partial()\n",
    "        predictions.append(pd.Series(model.predict(x_val).squeeze(), index=y_val.index))\n",
    "    return pd.concat(predictions)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T05:52:12.409529Z",
     "start_time": "2021-02-23T05:45:38.375Z"
    }
   },
   "outputs": [],
   "source": [
    "best_params = get_best_params()\n",
    "predictions = []\n",
    "for i, params in enumerate(best_params):\n",
    "    predictions.append(generate_predictions(**params).to_frame(i))\n",
    "\n",
    "predictions = pd.concat(predictions, axis=1)\n",
    "print(predictions.info())\n",
    "predictions.to_hdf(results_path / 'test_preds.h5', 'predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to further improve the results\n",
    "\n",
    "The relatively simple architecture yields some promising results. To further improve performance, you can\n",
    "- First and foremost, add new features and more data to the model\n",
    "- Expand the set of architectures to explore, including more or wider layers\n",
    "- Inspect the training progress and train for more epochs if the validation error continued to improve at 50 epochs\n",
    "\n",
    "Finally, you can use more sophisticated architectures, including Recurrent Neural Networks (RNN) and Convolutional Neural Networks that are well suited to sequential data, whereas vanilla feedforward NNs are not designed to capture the ordered nature of the features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282.222px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
