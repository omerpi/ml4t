{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 15, Word Embeddings, we discussed how to learn domain-specific word embeddings. Word2vec, and related learning algorithms, produce high-quality word vectors, but require large datasets. Hence, it is common that research groups share word vectors trained on large datasets, similar to the weights for pretrained deep learning models that we encountered in the section on transfer learning in the previous chapter.\n",
    "\n",
    "We are now going to illustrate how to use pretrained Global Vectors for Word Representation (GloVe) provided by the Stanford NLP group with the IMDB review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.262641Z",
     "start_time": "2021-02-23T15:49:34.570564Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.300492Z",
     "start_time": "2021-02-23T15:49:36.263937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.314770Z",
     "start_time": "2021-02-23T15:49:36.301918Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.323075Z",
     "start_time": "2021-02-23T15:49:36.316515Z"
    }
   },
   "outputs": [],
   "source": [
    "results_path = Path('results', 'sentiment_imdb')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load the IMDB dataset from the source for manual preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source: [Stanford IMDB Reviews Dataset](http://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dowload extract, and place the content in a newly created `data` folder so that your directory structure looks as follows:\n",
    "```\n",
    "19_recurrent_neural_nets\n",
    " |-data\n",
    "     |-aclimdb\n",
    "          |-train\n",
    "              |-neg\n",
    "              |-pos\n",
    "              ...\n",
    "          |-test\n",
    "          |-imdb.vocab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.330888Z",
     "start_time": "2021-02-23T15:49:36.324074Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path('data', 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.341442Z",
     "start_time": "2021-02-23T15:49:36.332179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = path.glob('**/*.txt')\n",
    "len(list(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.349477Z",
     "start_time": "2021-02-23T15:49:36.342666Z"
    }
   },
   "outputs": [],
   "source": [
    "files = path.glob('*/**/*.txt')\n",
    "outcomes = set()\n",
    "data = []\n",
    "for f in files:\n",
    "    if f.stem.startswith(('urls_', 'imdbEr')):\n",
    "        continue\n",
    "    _, _, data_set, outcome = f.parent.as_posix().split('/')\n",
    "    if outcome == 'unsup':\n",
    "        continue\n",
    "    data.append([data_set, int(outcome == 'pos'),\n",
    "                 f.read_text(encoding='latin1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.363471Z",
     "start_time": "2021-02-23T15:49:36.351799Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data, columns=['dataset', 'label', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.378155Z",
     "start_time": "2021-02-23T15:49:36.365187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   dataset  0 non-null      object\n",
      " 1   label    0 non-null      object\n",
      " 2   review   0 non-null      object\n",
      "dtypes: object(3)\n",
      "memory usage: 0.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.388250Z",
     "start_time": "2021-02-23T15:49:36.379440Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = data.loc[data.dataset=='train', ['label', 'review']]\n",
    "test_data = data.loc[data.dataset=='test', ['label', 'review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.397559Z",
     "start_time": "2021-02-23T15:49:36.389425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: label, dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.408428Z",
     "start_time": "2021-02-23T15:49:36.398760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: label, dtype: int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a tokenizer that we use to convert the text documents to integer-encoded sequences, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.415803Z",
     "start_time": "2021-02-23T15:49:36.410017Z"
    }
   },
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "t = Tokenizer(num_words=num_words, \n",
    "              lower=True, \n",
    "              oov_token=2)\n",
    "t.fit_on_texts(train_data.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.424150Z",
     "start_time": "2021-02-23T15:49:36.417134Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.432725Z",
     "start_time": "2021-02-23T15:49:36.425104Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_encoded = t.texts_to_sequences(train_data.review)\n",
    "test_data_encoded = t.texts_to_sequences(test_data.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.440991Z",
     "start_time": "2021-02-23T15:49:36.433833Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use the pad_sequences function to convert the list of lists (of unequal length) to stacked sets of padded and truncated arrays for both the train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.451625Z",
     "start_time": "2021-02-23T15:49:36.442549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded = pad_sequences(train_data_encoded, \n",
    "                            maxlen=max_length, \n",
    "                            padding='post',\n",
    "                           truncating='post')\n",
    "y_train = train_data['label']\n",
    "X_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:36.459294Z",
     "start_time": "2021-02-23T15:49:36.453286Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_padded = pad_sequences(test_data_encoded, \n",
    "                            maxlen=max_length, \n",
    "                            padding='post',\n",
    "                           truncating='post')\n",
    "y_test = test_data['label']\n",
    "X_test_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have downloaded and unzipped the GloVe data to the location indicated in the code, we now create a dictionary that maps GloVe tokens to 100-dimensional real-valued vectors, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:44.373282Z",
     "start_time": "2021-02-23T15:49:36.460711Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "glove_path = Path('..', 'data', 'glove', 'glove.6B.100d.txt')\n",
    "embeddings_index = dict()\n",
    "\n",
    "for line in glove_path.open(encoding='latin1'):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "    except:\n",
    "        continue\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:44.376743Z",
     "start_time": "2021-02-23T15:49:44.374204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 399,883 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Loaded {:,d} word vectors.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are around 340,000 word vectors that we use to create an embedding matrix that matches the vocabulary so that the RNN model can access embeddings by the token index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:44.388661Z",
     "start_time": "2021-02-23T15:49:44.378180Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:44.400211Z",
     "start_time": "2021-02-23T15:49:44.389704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between this and the RNN setup in the previous example is that we are going to pass the embedding matrix to the embedding layer and set it to non-trainable, so that the weights remain fixed during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:44.407974Z",
     "start_time": "2021-02-23T15:49:44.401833Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:45.206065Z",
     "start_time": "2021-02-23T15:49:44.408968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          200       \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 32)                12864     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 13,097\n",
      "Trainable params: 12,897\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn = Sequential([\n",
    "    Embedding(input_dim=vocab_size, \n",
    "              output_dim= embedding_size, \n",
    "              input_length=max_length,\n",
    "              weights=[embedding_matrix], \n",
    "              trainable=False),\n",
    "    GRU(units=32,  dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:45.234414Z",
     "start_time": "2021-02-23T15:49:45.207186Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn.compile(loss='binary_crossentropy',\n",
    "            optimizer='RMSProp',\n",
    "            metrics=['accuracy', \n",
    "                     tf.keras.metrics.AUC(name='AUC')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:45.238351Z",
     "start_time": "2021-02-23T15:49:45.235547Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn_path = (results_path / 'lstm.pretrained.h5').as_posix()\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=rnn_path,\n",
    "                               verbose=1,\n",
    "                               monitor='val_AUC',\n",
    "                               mode='max',\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:45.253290Z",
     "start_time": "2021-02-23T15:49:45.239807Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_AUC',\n",
    "                               patience=5,\n",
    "                               mode='max',\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:45.519840Z",
     "start_time": "2021-02-23T15:49:45.254656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'logs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-dc3cbd034de9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m training = rnn.fit(X_train_padded,\n\u001b[0m\u001b[1;32m      2\u001b[0m                    \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    validation_data=(X_test_padded,\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/ml4t-dl/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;31m# Run validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'logs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "training = rnn.fit(X_train_padded,\n",
    "                   y_train,\n",
    "                   batch_size=32,\n",
    "                   epochs=100,\n",
    "                   validation_data=(X_test_padded,\n",
    "                                    y_test),\n",
    "                   callbacks=[early_stopping,\n",
    "                              checkpointer],\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:45.521315Z",
     "start_time": "2021-02-23T15:49:34.659Z"
    }
   },
   "outputs": [],
   "source": [
    "y_score = rnn.predict(X_test_padded)\n",
    "roc_auc_score(y_score=y_score.squeeze(), y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:49:45.522036Z",
     "start_time": "2021-02-23T15:49:34.661Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(training.history)\n",
    "best_auc = df.val_AUC.max()\n",
    "best_acc = df.val_accuracy.max()\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(14,4))\n",
    "df.index = df.index.to_series().add(1)\n",
    "df[['AUC', 'val_AUC']].plot(ax=axes[0], \n",
    "                            title=f'AUC | Best: {best_auc:.4f}', \n",
    "                            legend=False, \n",
    "                            xlim=(1, 33),\n",
    "                            ylim=(.7, .95))\n",
    "\n",
    "axes[0].axvline(df.val_AUC.idxmax(), ls='--', lw=1, c='k')\n",
    "df[['accuracy', 'val_accuracy']].plot(ax=axes[1], \n",
    "                                              title=f'Accuracy | Best: {best_acc:.2%}', \n",
    "                                              legend=False, \n",
    "                                              xlim=(1, 33),\n",
    "                                      ylim=(.7, .9))\n",
    "axes[1].axvline(df.val_accuracy.idxmax(), ls='--', lw=1, c='k')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('AUC')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "fig.suptitle('Sentiment Analysis - Pretrained Vectors', fontsize=14)\n",
    "fig.legend(['Train', 'Validation'], loc='center right')\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=.9)\n",
    "fig.savefig(results_path / 'imdb_pretrained', dpi=300);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
