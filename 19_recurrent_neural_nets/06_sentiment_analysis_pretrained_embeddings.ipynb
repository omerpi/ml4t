{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 15, Word Embeddings, we discussed how to learn domain-specific word embeddings. Word2vec, and related learning algorithms, produce high-quality word vectors, but require large datasets. Hence, it is common that research groups share word vectors trained on large datasets, similar to the weights for pretrained deep learning models that we encountered in the section on transfer learning in the previous chapter.\n",
    "\n",
    "We are now going to illustrate how to use pretrained Global Vectors for Word Representation (GloVe) provided by the Stanford NLP group with the IMDB review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:54.294593Z",
     "start_time": "2021-02-23T17:14:52.919376Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:54.335324Z",
     "start_time": "2021-02-23T17:14:54.295781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:54.343605Z",
     "start_time": "2021-02-23T17:14:54.336878Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:54.356444Z",
     "start_time": "2021-02-23T17:14:54.344420Z"
    }
   },
   "outputs": [],
   "source": [
    "results_path = Path('results', 'sentiment_imdb')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load the IMDB dataset from the source for manual preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source: [Stanford IMDB Reviews Dataset](http://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dowload extract, and place the content in a newly created `data` folder so that your directory structure looks as follows:\n",
    "```\n",
    "19_recurrent_neural_nets\n",
    " |-data\n",
    "     |-aclimdb\n",
    "          |-train\n",
    "              |-neg\n",
    "              |-pos\n",
    "              ...\n",
    "          |-test\n",
    "          |-imdb.vocab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:54.364726Z",
     "start_time": "2021-02-23T17:14:54.357689Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path('data', 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:54.629329Z",
     "start_time": "2021-02-23T17:14:54.366279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50003"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = path.glob('**/*.txt')\n",
    "len(list(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:56.111490Z",
     "start_time": "2021-02-23T17:14:54.631782Z"
    }
   },
   "outputs": [],
   "source": [
    "files = path.glob('*/**/*.txt')\n",
    "outcomes = set()\n",
    "data = []\n",
    "for f in files:\n",
    "    if f.stem.startswith(('urls_', 'imdbEr')):\n",
    "        continue\n",
    "    _, _, data_set, outcome = f.parent.as_posix().split('/')\n",
    "    if outcome == 'unsup':\n",
    "        continue\n",
    "    data.append([data_set, int(outcome == 'pos'),\n",
    "                 f.read_text(encoding='latin1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:56.139785Z",
     "start_time": "2021-02-23T17:14:56.112577Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data, columns=['dataset', 'label', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:56.178373Z",
     "start_time": "2021-02-23T17:14:56.140713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   dataset  50000 non-null  object\n",
      " 1   label    50000 non-null  int64 \n",
      " 2   review   50000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:56.191606Z",
     "start_time": "2021-02-23T17:14:56.179291Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = data.loc[data.dataset=='train', ['label', 'review']]\n",
    "test_data = data.loc[data.dataset=='test', ['label', 'review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:56.196955Z",
     "start_time": "2021-02-23T17:14:56.192500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12500\n",
       "1    12500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:56.206787Z",
     "start_time": "2021-02-23T17:14:56.198458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12500\n",
       "1    12500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a tokenizer that we use to convert the text documents to integer-encoded sequences, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:58.986540Z",
     "start_time": "2021-02-23T17:14:56.208686Z"
    }
   },
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "t = Tokenizer(num_words=num_words, \n",
    "              lower=True, \n",
    "              oov_token=2)\n",
    "t.fit_on_texts(train_data.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:14:58.989919Z",
     "start_time": "2021-02-23T17:14:58.987445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88586"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:02.913481Z",
     "start_time": "2021-02-23T17:14:58.991167Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_encoded = t.texts_to_sequences(train_data.review)\n",
    "test_data_encoded = t.texts_to_sequences(test_data.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:02.916314Z",
     "start_time": "2021-02-23T17:15:02.914473Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use the pad_sequences function to convert the list of lists (of unequal length) to stacked sets of padded and truncated arrays for both the train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:03.128188Z",
     "start_time": "2021-02-23T17:15:02.917344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded = pad_sequences(train_data_encoded, \n",
    "                            maxlen=max_length, \n",
    "                            padding='post',\n",
    "                           truncating='post')\n",
    "y_train = train_data['label']\n",
    "X_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:03.343442Z",
     "start_time": "2021-02-23T17:15:03.130324Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_padded = pad_sequences(test_data_encoded, \n",
    "                            maxlen=max_length, \n",
    "                            padding='post',\n",
    "                           truncating='post')\n",
    "y_test = test_data['label']\n",
    "X_test_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have downloaded and unzipped the GloVe data to the location indicated in the code, we now create a dictionary that maps GloVe tokens to 100-dimensional real-valued vectors, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:09.664986Z",
     "start_time": "2021-02-23T17:15:03.344584Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "glove_path = Path('..', 'data', 'glove', 'glove.6B.100d.txt')\n",
    "embeddings_index = dict()\n",
    "\n",
    "for line in glove_path.open(encoding='latin1'):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "    except:\n",
    "        continue\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:09.668022Z",
     "start_time": "2021-02-23T17:15:09.665862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 399,883 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Loaded {:,d} word vectors.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are around 340,000 word vectors that we use to create an embedding matrix that matches the vocabulary so that the RNN model can access embeddings by the token index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:09.770408Z",
     "start_time": "2021-02-23T17:15:09.669142Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:09.774652Z",
     "start_time": "2021-02-23T17:15:09.772236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88586, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between this and the RNN setup in the previous example is that we are going to pass the embedding matrix to the embedding layer and set it to non-trainable, so that the weights remain fixed during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:09.783915Z",
     "start_time": "2021-02-23T17:15:09.775763Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:10.504198Z",
     "start_time": "2021-02-23T17:15:09.784990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          8858600   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 32)                12864     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 8,871,497\n",
      "Trainable params: 12,897\n",
      "Non-trainable params: 8,858,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn = Sequential([\n",
    "    Embedding(input_dim=vocab_size, \n",
    "              output_dim= embedding_size, \n",
    "              input_length=max_length,\n",
    "              weights=[embedding_matrix], \n",
    "              trainable=False),\n",
    "    GRU(units=32,  dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:10.518063Z",
     "start_time": "2021-02-23T17:15:10.505197Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn.compile(loss='binary_crossentropy',\n",
    "            optimizer='RMSProp',\n",
    "            metrics=['accuracy', \n",
    "                     tf.keras.metrics.AUC(name='AUC')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:10.524265Z",
     "start_time": "2021-02-23T17:15:10.519172Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn_path = (results_path / 'lstm.pretrained.h5').as_posix()\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=rnn_path,\n",
    "                               verbose=1,\n",
    "                               monitor='val_AUC',\n",
    "                               mode='max',\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T17:15:10.532489Z",
     "start_time": "2021-02-23T17:15:10.525759Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_AUC',\n",
    "                               patience=5,\n",
    "                               mode='max',\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-23T17:14:52.960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.6505 - accuracy: 0.6087 - AUC: 0.6565\n",
      "Epoch 00001: val_AUC improved from -inf to 0.80524, saving model to results/sentiment_imdb/lstm.pretrained.h5\n",
      "782/782 [==============================] - 111s 141ms/step - loss: 0.6505 - accuracy: 0.6087 - AUC: 0.6565 - val_loss: 0.7026 - val_accuracy: 0.6586 - val_AUC: 0.8052\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.7570 - AUC: 0.8316\n",
      "Epoch 00002: val_AUC improved from 0.80524 to 0.86949, saving model to results/sentiment_imdb/lstm.pretrained.h5\n",
      "782/782 [==============================] - 111s 143ms/step - loss: 0.5053 - accuracy: 0.7570 - AUC: 0.8316 - val_loss: 0.4681 - val_accuracy: 0.7776 - val_AUC: 0.8695\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4538 - accuracy: 0.7872 - AUC: 0.8679\n",
      "Epoch 00003: val_AUC improved from 0.86949 to 0.88737, saving model to results/sentiment_imdb/lstm.pretrained.h5\n",
      "782/782 [==============================] - 117s 149ms/step - loss: 0.4538 - accuracy: 0.7872 - AUC: 0.8679 - val_loss: 0.4352 - val_accuracy: 0.7931 - val_AUC: 0.8874\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.8000 - AUC: 0.8823\n",
      "Epoch 00004: val_AUC improved from 0.88737 to 0.89225, saving model to results/sentiment_imdb/lstm.pretrained.h5\n",
      "782/782 [==============================] - 109s 139ms/step - loss: 0.4303 - accuracy: 0.8000 - AUC: 0.8823 - val_loss: 0.4404 - val_accuracy: 0.7947 - val_AUC: 0.8922\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4138 - accuracy: 0.8062 - AUC: 0.8915\n",
      "Epoch 00005: val_AUC improved from 0.89225 to 0.89876, saving model to results/sentiment_imdb/lstm.pretrained.h5\n",
      "782/782 [==============================] - 109s 139ms/step - loss: 0.4138 - accuracy: 0.8062 - AUC: 0.8915 - val_loss: 0.4284 - val_accuracy: 0.7994 - val_AUC: 0.8988\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4064 - accuracy: 0.8123 - AUC: 0.8959\n",
      "Epoch 00006: val_AUC improved from 0.89876 to 0.90227, saving model to results/sentiment_imdb/lstm.pretrained.h5\n",
      "782/782 [==============================] - 109s 139ms/step - loss: 0.4064 - accuracy: 0.8123 - AUC: 0.8959 - val_loss: 0.4015 - val_accuracy: 0.8176 - val_AUC: 0.9023\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3959 - accuracy: 0.8201 - AUC: 0.9016\n",
      "Epoch 00007: val_AUC improved from 0.90227 to 0.90520, saving model to results/sentiment_imdb/lstm.pretrained.h5\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.3959 - accuracy: 0.8201 - AUC: 0.9016 - val_loss: 0.4228 - val_accuracy: 0.8072 - val_AUC: 0.9052\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3884 - accuracy: 0.8230 - AUC: 0.9054\n",
      "Epoch 00008: val_AUC improved from 0.90520 to 0.90554, saving model to results/sentiment_imdb/lstm.pretrained.h5\n",
      "782/782 [==============================] - 118s 151ms/step - loss: 0.3884 - accuracy: 0.8230 - AUC: 0.9054 - val_loss: 0.4152 - val_accuracy: 0.8064 - val_AUC: 0.9055\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3814 - accuracy: 0.8264 - AUC: 0.9090\n",
      "Epoch 00009: val_AUC improved from 0.90554 to 0.90921, saving model to results/sentiment_imdb/lstm.pretrained.h5\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.3814 - accuracy: 0.8264 - AUC: 0.9090 - val_loss: 0.3939 - val_accuracy: 0.8167 - val_AUC: 0.9092\n",
      "Epoch 10/100\n",
      "637/782 [=======================>......] - ETA: 17s - loss: 0.3778 - accuracy: 0.8276 - AUC: 0.9107"
     ]
    }
   ],
   "source": [
    "training = rnn.fit(X_train_padded,\n",
    "                   y_train,\n",
    "                   batch_size=32,\n",
    "                   epochs=100,\n",
    "                   validation_data=(X_test_padded,\n",
    "                                    y_test),\n",
    "                   callbacks=[early_stopping,\n",
    "                              checkpointer],\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-23T17:14:52.961Z"
    }
   },
   "outputs": [],
   "source": [
    "y_score = rnn.predict(X_test_padded)\n",
    "roc_auc_score(y_score=y_score.squeeze(), y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-23T17:14:52.963Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(training.history)\n",
    "best_auc = df.val_AUC.max()\n",
    "best_acc = df.val_accuracy.max()\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(14,4))\n",
    "df.index = df.index.to_series().add(1)\n",
    "df[['AUC', 'val_AUC']].plot(ax=axes[0], \n",
    "                            title=f'AUC | Best: {best_auc:.4f}', \n",
    "                            legend=False, \n",
    "                            xlim=(1, 33),\n",
    "                            ylim=(.7, .95))\n",
    "\n",
    "axes[0].axvline(df.val_AUC.idxmax(), ls='--', lw=1, c='k')\n",
    "df[['accuracy', 'val_accuracy']].plot(ax=axes[1], \n",
    "                                              title=f'Accuracy | Best: {best_acc:.2%}', \n",
    "                                              legend=False, \n",
    "                                              xlim=(1, 33),\n",
    "                                      ylim=(.7, .9))\n",
    "axes[1].axvline(df.val_accuracy.idxmax(), ls='--', lw=1, c='k')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('AUC')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "fig.suptitle('Sentiment Analysis - Pretrained Vectors', fontsize=14)\n",
    "fig.legend(['Train', 'Validation'], loc='center right')\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=.9)\n",
    "fig.savefig(results_path / 'imdb_pretrained', dpi=300);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
