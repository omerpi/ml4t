{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 15, Word Embeddings, we discussed how to learn domain-specific word embeddings. Word2vec, and related learning algorithms, produce high-quality word vectors, but require large datasets. Hence, it is common that research groups share word vectors trained on large datasets, similar to the weights for pretrained deep learning models that we encountered in the section on transfer learning in the previous chapter.\n",
    "\n",
    "We are now going to illustrate how to use pretrained Global Vectors for Word Representation (GloVe) provided by the Stanford NLP group with the IMDB review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:38.648521Z",
     "start_time": "2020-03-25T04:20:37.394729Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, date\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Input, concatenate, Embedding, Reshape\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:38.653000Z",
     "start_time": "2020-03-25T04:20:38.649521Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load the IMDB dataset from the source for manual preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source: [Stanford IMDB Reviews Dataset](http://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:38.667094Z",
     "start_time": "2020-03-25T04:20:38.654020Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path('data/aclImdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:38.984717Z",
     "start_time": "2020-03-25T04:20:38.668093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = path.glob('**/*.txt')\n",
    "len(list(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:40.290062Z",
     "start_time": "2020-03-25T04:20:38.985830Z"
    }
   },
   "outputs": [],
   "source": [
    "files = path.glob('*/**/*.txt')\n",
    "data = []\n",
    "for f in files:\n",
    "    _, _, data_set, outcome = str(f.parent).split('/')\n",
    "    data.append([data_set, int(outcome=='pos'), f.read_text(encoding='latin1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:40.324137Z",
     "start_time": "2020-03-25T04:20:40.290929Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data, columns=['dataset', 'label', 'review']).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:40.340928Z",
     "start_time": "2020-03-25T04:20:40.325001Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = data.loc[data.dataset=='train', ['label', 'review']]\n",
    "test_data = data.loc[data.dataset=='test', ['label', 'review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:40.346143Z",
     "start_time": "2020-03-25T04:20:40.342222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:40.356076Z",
     "start_time": "2020-03-25T04:20:40.347254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a tokenizer that we use to convert the text documents to integer-encoded sequences, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:43.067473Z",
     "start_time": "2020-03-25T04:20:40.356875Z"
    }
   },
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "t = Tokenizer(num_words=num_words, \n",
    "              lower=True, \n",
    "              oov_token=2)\n",
    "t.fit_on_texts(train_data.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:43.071382Z",
     "start_time": "2020-03-25T04:20:43.068505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88586"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:47.083889Z",
     "start_time": "2020-03-25T04:20:43.072282Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_encoded = t.texts_to_sequences(train_data.review)\n",
    "test_data_encoded = t.texts_to_sequences(test_data.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:47.086412Z",
     "start_time": "2020-03-25T04:20:47.084785Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use the pad_sequences function to convert the list of lists (of unequal length) to stacked sets of padded and truncated arrays for both the train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:47.280381Z",
     "start_time": "2020-03-25T04:20:47.087273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded = pad_sequences(train_data_encoded, \n",
    "                            maxlen=max_length, \n",
    "                            padding='post',\n",
    "                           truncating='post')\n",
    "y_train = train_data['label']\n",
    "X_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:47.484225Z",
     "start_time": "2020-03-25T04:20:47.281213Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_padded = pad_sequences(test_data_encoded, \n",
    "                            maxlen=max_length, \n",
    "                            padding='post',\n",
    "                           truncating='post')\n",
    "y_test = test_data['label']\n",
    "X_test_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have downloaded and unzipped the GloVe data to the location indicated in the code, we now create a dictionary that maps GloVe tokens to 100-dimensional real-valued vectors, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:54.219704Z",
     "start_time": "2020-03-25T04:20:47.485070Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "glove_path = Path('data/glove/glove.6B.100d.txt')\n",
    "embeddings_index = dict()\n",
    "\n",
    "for line in glove_path.open(encoding='latin1'):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "    except:\n",
    "        continue\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:54.222636Z",
     "start_time": "2020-03-25T04:20:54.220565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 399,883 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Loaded {:,d} word vectors.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are around 340,000 word vectors that we use to create an embedding matrix that matches the vocabulary so that the RNN model can access embeddings by the token index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:54.329212Z",
     "start_time": "2020-03-25T04:20:54.223515Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:54.332609Z",
     "start_time": "2020-03-25T04:20:54.330086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88586, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between this and the RNN setup in the previous example is that we are going to pass the embedding matrix to the embedding layer and set it to non-trainable, so that the weights remain fixed during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:54.342298Z",
     "start_time": "2020-03-25T04:20:54.333551Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:55.083691Z",
     "start_time": "2020-03-25T04:20:54.343902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          8858600   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 32)                12864     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 8,871,497\n",
      "Trainable params: 12,897\n",
      "Non-trainable params: 8,858,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn = Sequential([\n",
    "    Embedding(input_dim=vocab_size, \n",
    "              output_dim= embedding_size, \n",
    "              input_length=max_length,\n",
    "              weights=[embedding_matrix], \n",
    "              trainable=False),\n",
    "    GRU(units=32,  dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:55.151346Z",
     "start_time": "2020-03-25T04:20:55.084753Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn.compile(loss='binary_crossentropy',\n",
    "            optimizer='RMSProp',\n",
    "            metrics=['accuracy', tf.keras.metrics.AUC(name='AUC')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T04:20:55.156793Z",
     "start_time": "2020-03-25T04:20:55.153986Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_AUC',\n",
    "                               patience=5,\n",
    "                               mode='max',\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T05:06:27.473491Z",
     "start_time": "2020-03-25T04:20:55.158047Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/100\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.6581 - accuracy: 0.5944 - AUC: 0.6392 - val_loss: 0.5429 - val_accuracy: 0.7361 - val_AUC: 0.8090\n",
      "Epoch 2/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.5300 - accuracy: 0.7418 - AUC: 0.8124 - val_loss: 0.4712 - val_accuracy: 0.7712 - val_AUC: 0.8657\n",
      "Epoch 3/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.4730 - accuracy: 0.7763 - AUC: 0.8554 - val_loss: 0.4392 - val_accuracy: 0.7900 - val_AUC: 0.8804\n",
      "Epoch 4/100\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.4511 - accuracy: 0.7860 - AUC: 0.8692 - val_loss: 0.4242 - val_accuracy: 0.7988 - val_AUC: 0.8895\n",
      "Epoch 5/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.4364 - accuracy: 0.7989 - AUC: 0.8789 - val_loss: 0.4083 - val_accuracy: 0.8082 - val_AUC: 0.8945\n",
      "Epoch 6/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.4257 - accuracy: 0.8028 - AUC: 0.8847 - val_loss: 0.4033 - val_accuracy: 0.8110 - val_AUC: 0.8979\n",
      "Epoch 7/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.4128 - accuracy: 0.8073 - AUC: 0.8924 - val_loss: 0.3994 - val_accuracy: 0.8138 - val_AUC: 0.9005\n",
      "Epoch 8/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.4068 - accuracy: 0.8110 - AUC: 0.8954 - val_loss: 0.3963 - val_accuracy: 0.8152 - val_AUC: 0.9019\n",
      "Epoch 9/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3982 - accuracy: 0.8185 - AUC: 0.9004 - val_loss: 0.3923 - val_accuracy: 0.8177 - val_AUC: 0.9035\n",
      "Epoch 10/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3932 - accuracy: 0.8203 - AUC: 0.9029 - val_loss: 0.3881 - val_accuracy: 0.8199 - val_AUC: 0.9050\n",
      "Epoch 11/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3850 - accuracy: 0.8244 - AUC: 0.9070 - val_loss: 0.3878 - val_accuracy: 0.8201 - val_AUC: 0.9055\n",
      "Epoch 12/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3764 - accuracy: 0.8305 - AUC: 0.9114 - val_loss: 0.3951 - val_accuracy: 0.8170 - val_AUC: 0.9077\n",
      "Epoch 13/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3711 - accuracy: 0.8313 - AUC: 0.9142 - val_loss: 0.4106 - val_accuracy: 0.8110 - val_AUC: 0.9070\n",
      "Epoch 14/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3701 - accuracy: 0.8344 - AUC: 0.9147 - val_loss: 0.3866 - val_accuracy: 0.8222 - val_AUC: 0.9097\n",
      "Epoch 15/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3664 - accuracy: 0.8356 - AUC: 0.9163 - val_loss: 0.3882 - val_accuracy: 0.8212 - val_AUC: 0.9103\n",
      "Epoch 16/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3614 - accuracy: 0.8366 - AUC: 0.9187 - val_loss: 0.3809 - val_accuracy: 0.8267 - val_AUC: 0.9098\n",
      "Epoch 17/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3579 - accuracy: 0.8406 - AUC: 0.9206 - val_loss: 0.3792 - val_accuracy: 0.8247 - val_AUC: 0.9104\n",
      "Epoch 18/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3540 - accuracy: 0.8417 - AUC: 0.9221 - val_loss: 0.4038 - val_accuracy: 0.8126 - val_AUC: 0.9108\n",
      "Epoch 19/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3522 - accuracy: 0.8431 - AUC: 0.9230 - val_loss: 0.3779 - val_accuracy: 0.8290 - val_AUC: 0.9116\n",
      "Epoch 20/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3503 - accuracy: 0.8427 - AUC: 0.9239 - val_loss: 0.3785 - val_accuracy: 0.8281 - val_AUC: 0.9122\n",
      "Epoch 21/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3494 - accuracy: 0.8440 - AUC: 0.9242 - val_loss: 0.3769 - val_accuracy: 0.8269 - val_AUC: 0.9117\n",
      "Epoch 22/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3436 - accuracy: 0.8470 - AUC: 0.9270 - val_loss: 0.3754 - val_accuracy: 0.8282 - val_AUC: 0.9117\n",
      "Epoch 23/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3430 - accuracy: 0.8455 - AUC: 0.9271 - val_loss: 0.3761 - val_accuracy: 0.8291 - val_AUC: 0.9126\n",
      "Epoch 24/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3406 - accuracy: 0.8496 - AUC: 0.9284 - val_loss: 0.3915 - val_accuracy: 0.8224 - val_AUC: 0.9122\n",
      "Epoch 25/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3376 - accuracy: 0.8498 - AUC: 0.9295 - val_loss: 0.3774 - val_accuracy: 0.8299 - val_AUC: 0.9129\n",
      "Epoch 26/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3371 - accuracy: 0.8505 - AUC: 0.9298 - val_loss: 0.3771 - val_accuracy: 0.8313 - val_AUC: 0.9132\n",
      "Epoch 27/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3367 - accuracy: 0.8511 - AUC: 0.9299 - val_loss: 0.3778 - val_accuracy: 0.8293 - val_AUC: 0.9123\n",
      "Epoch 28/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3359 - accuracy: 0.8525 - AUC: 0.9305 - val_loss: 0.3784 - val_accuracy: 0.8286 - val_AUC: 0.9132\n",
      "Epoch 29/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3315 - accuracy: 0.8526 - AUC: 0.9322 - val_loss: 0.3772 - val_accuracy: 0.8304 - val_AUC: 0.9128\n",
      "Epoch 30/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3298 - accuracy: 0.8540 - AUC: 0.9328 - val_loss: 0.3776 - val_accuracy: 0.8314 - val_AUC: 0.9135\n",
      "Epoch 31/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3253 - accuracy: 0.8540 - AUC: 0.9346 - val_loss: 0.3898 - val_accuracy: 0.8226 - val_AUC: 0.9136\n",
      "Epoch 32/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3280 - accuracy: 0.8542 - AUC: 0.9336 - val_loss: 0.3782 - val_accuracy: 0.8310 - val_AUC: 0.9126\n",
      "Epoch 33/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3254 - accuracy: 0.8567 - AUC: 0.9348 - val_loss: 0.3846 - val_accuracy: 0.8289 - val_AUC: 0.9127\n",
      "Epoch 34/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3247 - accuracy: 0.8576 - AUC: 0.9351 - val_loss: 0.3801 - val_accuracy: 0.8297 - val_AUC: 0.9133\n",
      "Epoch 35/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3226 - accuracy: 0.8581 - AUC: 0.9360 - val_loss: 0.4000 - val_accuracy: 0.8195 - val_AUC: 0.9127\n",
      "Epoch 36/100\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3234 - accuracy: 0.8573 - AUC: 0.9355 - val_loss: 0.3863 - val_accuracy: 0.8284 - val_AUC: 0.9121\n"
     ]
    }
   ],
   "source": [
    "training = rnn.fit(X_train_padded,\n",
    "                  y_train,\n",
    "                  batch_size=32,\n",
    "                  epochs=100,\n",
    "                  validation_data=(X_test_padded, y_test),\n",
    "                  callbacks=[early_stopping],\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T05:06:39.899755Z",
     "start_time": "2020-03-25T05:06:27.474413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9136563328"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score = rnn.predict(X_test_padded)\n",
    "roc_auc_score(y_score=y_score.squeeze(), y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T05:06:39.903220Z",
     "start_time": "2020-03-25T05:06:39.900738Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(training.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T05:06:39.919918Z",
     "start_time": "2020-03-25T05:06:39.904023Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('lstm_sa_pretrained.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
