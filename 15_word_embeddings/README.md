# Chapter 14: word2ved

## How Word Embeddings encode Semantics

### How neural language models learn usage in context
### How neural language models learn usage in context

## Word Vectors from SEC Filings using gensim

- [2013-2016 Cleaned/Parsed 10-K Filings with the SEC](https://data.world/jumpyaf/2013-2016-cleaned-parsed-10-k-filings-with-the-sec)
- [Stock Market Predictions with Natural Language Deep Learning](https://www.microsoft.com/developerblog/2017/12/04/predicting-stock-performance-deep-learning/)

## Sentiment Analysis with Doc2Vec

## Bonus: word2vec for translation

- [Exploiting Similarities among Languages for Machine Translation](https://arxiv.org/abs/1309.4168), Tomas Mikolov, Quoc V. Le, Ilya Sutskever, arxiv 2013
- [Word and Phrase Translation with word2vec](https://arxiv.org/abs/1705.03127), Stefan Jansen, arxiv, 2017

### Resources

- [GloVe: Global Vectors for Word Representation](https://github.com/stanfordnlp/GloVe)
- [Common Crawl Data](http://commoncrawl.org/the-data/)
- [word2vec analogy samples](https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt)
- [spaCy word vectors and semantic similarity](https://spacy.io/usage/vectors-similarity)
- [2013-2016 Cleaned/Parsed 10-K Filings with the SEC](https://data.world/jumpyaf/2013-2016-cleaned-parsed-10-k-filings-with-the-sec)