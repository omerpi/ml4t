{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting with zipline - Pipeline API with Custom Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Pipeline API](https://www.quantopian.com/docs/user-guide/tools/pipeline) facilitates the definition and computation of alpha factors for a cross-section of securities from historical data. The Pipeline significantly improves efficiency because it optimizes computations over the entire backtest period rather than tackling each event separately. In other words, it continues to follow an event-driven architecture but vectorizes the computation of factors where possible. \n",
    "\n",
    "A Pipeline uses Factors, Filters, and Classifiers classes to define computations that produce columns in a table with PIT values for a set of securities. Factors take one or more input arrays of historical bar data and produce one or more outputs for each security. There are numerous built-in factors, and you can also design your own `CustomFactor` computations.\n",
    "\n",
    "The following figure depicts how loading the data using the `DataFrameLoader`, computing the predictive `MLSignal` using the Pipeline API, and various scheduled activities integrate with the overall trading algorithm executed via the `run_algorithm()` function. We go over the details and the corresponding code in this section.\n",
    "\n",
    "![The Pipeline Workflow](../../assets/zip_pipe_flow.png)\n",
    "\n",
    "You need to register your Pipeline with the `initialize()` method and can then execute it at each time step or on a custom schedule. Zipline provides numerous built-in computations such as moving averages or Bollinger Bands that can be used to quickly compute standard factors, but it also allows for the creation of custom factors as we will illustrate next. \n",
    "\n",
    "Most importantly, the Pipeline API renders alpha factor research modular because it separates the alpha factor computation from the remainder of the algorithm, including the placement and execution of trade orders and the bookkeeping of portfolio holdings, values, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to combine the daily return predictions with the OHCLV data from our Quandl bundle and then to go long on up to 10 equities with the highest predicted returns and short on those with the lowest predicted returns, requiring at least five stocks on either side similar to the backtrader example above. See comments in the notebook for implementation details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:19.409511Z",
     "start_time": "2020-06-17T22:08:19.407621Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:20.284479Z",
     "start_time": "2020-06-17T22:08:19.410600Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "from logbook import Logger, StderrHandler, INFO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from zipline import run_algorithm\n",
    "from zipline.api import (attach_pipeline,\n",
    "                         pipeline_output,\n",
    "                         date_rules,\n",
    "                         time_rules,\n",
    "                         record,\n",
    "                         schedule_function,\n",
    "                         commission,\n",
    "                         slippage,\n",
    "                         set_slippage,\n",
    "                         set_commission,\n",
    "                         order_target,\n",
    "                         order_target_percent)\n",
    "from zipline.data import bundles\n",
    "from zipline.utils.run_algo import load_extensions\n",
    "from zipline.pipeline import Pipeline, CustomFactor\n",
    "from zipline.pipeline.data import Column, DataSet\n",
    "from zipline.pipeline.domain import US_EQUITIES\n",
    "from zipline.pipeline.filters import StaticAssets\n",
    "from zipline.pipeline.loaders import USEquityPricingLoader\n",
    "from zipline.pipeline.loaders.frame import DataFrameLoader\n",
    "\n",
    "import pyfolio as pf\n",
    "from pyfolio.plotting import plot_rolling_returns, plot_rolling_sharpe\n",
    "from pyfolio.timeseries import forecast_cone_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:20.288443Z",
     "start_time": "2020-06-17T22:08:20.286099Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load zipline extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only need this in notebook to find bundle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:20.306754Z",
     "start_time": "2020-06-17T22:08:20.290086Z"
    }
   },
   "outputs": [],
   "source": [
    "load_extensions(default=True,\n",
    "                extensions=[],\n",
    "                strict=True,\n",
    "                environ=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:20.314654Z",
     "start_time": "2020-06-17T22:08:20.307829Z"
    }
   },
   "outputs": [],
   "source": [
    "log_handler = StderrHandler(format_string='[{record.time:%Y-%m-%d %H:%M:%S.%f}]: ' +\n",
    "                            '{record.level_name}: {record.func_name}: {record.message}',\n",
    "                            level=INFO)\n",
    "log_handler.push_application()\n",
    "log = Logger('Algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algo Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan to hold up to 20 long and 20 short positions whenever there are at least 10 on either side that meet the criteria (positive/negative prediction for long/short position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:20.322671Z",
     "start_time": "2020-06-17T22:08:20.315676Z"
    }
   },
   "outputs": [],
   "source": [
    "N_LONGS = 20\n",
    "N_SHORTS = 20\n",
    "MIN_POSITIONS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quandl Wiki Bundle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Wiki Quandl `bundle` data that we ingested earlier using `zipline ingest`. This gives us access to the security SID values, among other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:20.550053Z",
     "start_time": "2020-06-17T22:08:20.323663Z"
    }
   },
   "outputs": [],
   "source": [
    "bundle_data = bundles.load('quandl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our predictions for the 2015-17 period and extract the Zipline IDs for the ~250 stocks in our universe during this period using the `bundle.asset_finder.lookup_symbols()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:20.554029Z",
     "start_time": "2020-06-17T22:08:20.551089Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_predictions(bundle):\n",
    "    predictions = pd.read_hdf('../00_data/backtest.h5', 'data')[['predicted']].dropna()\n",
    "    tickers = predictions.index.get_level_values(0).unique().tolist()\n",
    "\n",
    "    assets = bundle.asset_finder.lookup_symbols(tickers, as_of_date=None)\n",
    "    predicted_sids = pd.Int64Index([asset.sid for asset in assets])\n",
    "    ticker_map = dict(zip(tickers, predicted_sids))\n",
    "    return (predictions\n",
    "            .unstack('ticker')\n",
    "            .rename(columns=ticker_map)\n",
    "            .predicted\n",
    "            .tz_localize('UTC')), assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:21.080915Z",
     "start_time": "2020-06-17T22:08:20.555041Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File ../00_data/backtest.h5 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-538a92167a3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbundle_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-084d3e0e1578>\u001b[0m in \u001b[0;36mload_predictions\u001b[0;34m(bundle)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../00_data/backtest.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtickers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0massets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masset_finder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_symbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_of_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ml4t-zipline/lib/python3.5/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             raise compat.FileNotFoundError(\n\u001b[0;32m--> 347\u001b[0;31m                 'File %s does not exist' % path_or_buf)\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File ../00_data/backtest.h5 does not exist"
     ]
    }
   ],
   "source": [
    "predictions, assets = load_predictions(bundle_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To merge additional columns with our bundle, we define a custom `SignalData` class that inherits from `zipline.pipeline.DataSset` and contains a single `zipline.pipeline.Column` of type `float` and has the domain `US_EQUITIES`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:21.083788Z",
     "start_time": "2020-06-17T22:08:21.081957Z"
    }
   },
   "outputs": [],
   "source": [
    "class SignalData(DataSet):\n",
    "    predictions = Column(dtype=float)\n",
    "    domain = US_EQUITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Pipeline Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the bundle’s OHLCV data can rely on the built-in `USEquityPricingLoader`, we need to define our own `zipline.pipeline.loaders.frame.DataFrameLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:21.102594Z",
     "start_time": "2020-06-17T22:08:21.084962Z"
    }
   },
   "outputs": [],
   "source": [
    "signal_loader = {SignalData.predictions:\n",
    "                     DataFrameLoader(SignalData.predictions, predictions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we need to slightly modify the Zipline library’s source code to bypass the assumption that we will only load price data. To this end, we will add a `custom_loader` parameter to the `run_algorithm` and ensure that this loader is used when the `Pipeline` needs one of `SignalData`’s `Column` instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Pipeline is going to have two Boolean columns that identify the assets we would like to trade as long and short positions. \n",
    "\n",
    "To get there, we first define a `CustomFactor` called `MLSignal` that just receives the current `SignalData.predictions`. The motivation is to allow us to use some of the convenient `Factor` methods designed to rank and filter securities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ML Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:21.111584Z",
     "start_time": "2020-06-17T22:08:21.105048Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLSignal(CustomFactor):\n",
    "    \"\"\"Converting signals to Factor\n",
    "        so we can rank and filter in Pipeline\"\"\"\n",
    "    inputs = [SignalData.predictions]\n",
    "    window_length = 1\n",
    "\n",
    "    def compute(self, today, assets, out, preds):\n",
    "        out[:] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a `compute_signals()` that returns a `zipline.pipeline.Pipeline` which filters the assets that meet our long/short criteria. We will call ths function periodically while executing the backtest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specifically, we set up our Pipeline by instantiating the `CustomFactor` that requires no arguments other than the defaults. We combine its `top()` and `bottom()` methods with a filter to select the highest positive and lowest negative predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:21.120083Z",
     "start_time": "2020-06-17T22:08:21.113537Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_signals():\n",
    "    signals = MLSignal()\n",
    "#     predictions = SignalData.predictions.latest\n",
    "    return Pipeline(columns={\n",
    "        'longs' : signals.top(N_LONGS, mask=signals > 0),\n",
    "        'shorts': signals.bottom(N_SHORTS, mask=signals < 0)},\n",
    "            screen=StaticAssets(assets)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `initialize()` function is part of the Algorithm API. It permits us to add entries to the `context` dictionary available to all backtest components, set parameters like commission and slippage, and schedule functions. We also attach our Pipeline to the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:21.127912Z",
     "start_time": "2020-06-17T22:08:21.121012Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize(context):\n",
    "    \"\"\"\n",
    "    Called once at the start of the algorithm.\n",
    "    \"\"\"\n",
    "    context.n_longs = N_LONGS\n",
    "    context.n_shorts = N_SHORTS\n",
    "    context.min_positions = MIN_POSITIONS\n",
    "    context.universe = assets\n",
    "\n",
    "    set_slippage(slippage.FixedSlippage(spread=0.00))\n",
    "    set_commission(commission.PerShare(cost=0, min_trade_cost=0))\n",
    "\n",
    "    schedule_function(rebalance,\n",
    "                      date_rules.every_day(),\n",
    "                      time_rules.market_open(hours=1, minutes=30))\n",
    "\n",
    "    schedule_function(record_vars,\n",
    "                      date_rules.every_day(),\n",
    "                      time_rules.market_close())\n",
    "\n",
    "    pipeline = compute_signals()\n",
    "    attach_pipeline(pipeline, 'signals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get daily Pipeline results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm calls the `before_trading_start()` function every day before market opens and we use it to obtain the current pipeline values, i.e., the assets suggested for long and short positions based on the ML model predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:21.138949Z",
     "start_time": "2020-06-17T22:08:21.128880Z"
    }
   },
   "outputs": [],
   "source": [
    "def before_trading_start(context, data):\n",
    "    \"\"\"\n",
    "    Called every day before market open.\n",
    "    \"\"\"\n",
    "    output = pipeline_output('signals')\n",
    "    context.trades = (output['longs'].astype(int)\n",
    "                      .append(output['shorts'].astype(int).mul(-1))\n",
    "                      .reset_index()\n",
    "                      .drop_duplicates()\n",
    "                      .set_index('index')\n",
    "                      .squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Rebalancing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rebalance()` function takes care of adjusting the portfolio positions to reflect the target long and short positions implied by the model forecasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:21.146848Z",
     "start_time": "2020-06-17T22:08:21.139968Z"
    }
   },
   "outputs": [],
   "source": [
    "def rebalance(context, data):\n",
    "    \"\"\"\n",
    "    Execute orders according to schedule_function() date & time rules.\n",
    "    \"\"\"\n",
    "    trades = defaultdict(list)\n",
    "\n",
    "    for stock, trade in context.trades.items():\n",
    "        if not trade:\n",
    "            order_target(stock, 0)\n",
    "        else:\n",
    "            trades[trade].append(stock)\n",
    "    context.longs, context.shorts = len(trades[1]), len(trades[-1])\n",
    "    if context.longs > context.min_positions and context.shorts > context.min_positions:\n",
    "        for stock in trades[-1]:\n",
    "            order_target_percent(stock, -1 / context.shorts)\n",
    "        for stock in trades[1]:\n",
    "            order_target_percent(stock, 1 / context.longs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Data Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `record_vars()` logs information to the `pd.DataFrame` returned by `run_algorithm()` as scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:21.154219Z",
     "start_time": "2020-06-17T22:08:21.149111Z"
    }
   },
   "outputs": [],
   "source": [
    "def record_vars(context, data):\n",
    "    \"\"\"\n",
    "    Plot variables at the end of each day.\n",
    "    \"\"\"\n",
    "    record(leverage=context.account.leverage,\n",
    "           longs=context.longs,\n",
    "           shorts=context.shorts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have defined all ingredients for the algorithm and are ready to call `run_algorithm()` with the desired `start` and `end` dates, references to the various functions we just created, and the `custom_loader` to ensure our model predictions are available to the backtest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:21.161947Z",
     "start_time": "2020-06-17T22:08:21.155318Z"
    }
   },
   "outputs": [],
   "source": [
    "dates = predictions.index.get_level_values('date')\n",
    "start_date = dates.min()\n",
    "end_date = (dates.max() + pd.DateOffset(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:21.177892Z",
     "start_time": "2020-06-17T22:08:21.162768Z"
    }
   },
   "outputs": [],
   "source": [
    "start_date, end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:45.122201Z",
     "start_time": "2020-06-17T22:08:21.178897Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "results = run_algorithm(start=start_date,\n",
    "                       end=end_date,\n",
    "                       initialize=initialize,\n",
    "                       before_trading_start=before_trading_start,\n",
    "                       capital_base=1e6,\n",
    "                       data_frequency='daily',\n",
    "                       bundle='quandl',\n",
    "                       custom_loader=signal_loader) # need to modify zipline\n",
    "\n",
    "print('Duration: {:.2f}s'.format(time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis with PyFolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the results using `pyfolio` tearsheets or its various `pyfolio.plotting` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:45.842477Z",
     "start_time": "2020-06-17T22:08:45.123324Z"
    }
   },
   "outputs": [],
   "source": [
    "returns, positions, transactions = pf.utils.extract_rets_pos_txn_from_zipline(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:46.385123Z",
     "start_time": "2020-06-17T22:08:45.845509Z"
    }
   },
   "outputs": [],
   "source": [
    "benchmark = web.DataReader('SP500', 'fred', '2014', '2018').squeeze()\n",
    "benchmark = benchmark.pct_change().tz_localize('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:46.391907Z",
     "start_time": "2020-06-17T22:08:46.388426Z"
    }
   },
   "outputs": [],
   "source": [
    "LIVE_DATE = '2017-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:08:46.853921Z",
     "start_time": "2020-06-17T22:08:46.394983Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(16, 5))\n",
    "plot_rolling_returns(returns,\n",
    "                     factor_returns=benchmark,\n",
    "                     live_start_date=LIVE_DATE,\n",
    "                     logy=False,\n",
    "                     cone_std=2,\n",
    "                     legend_loc='best',\n",
    "                     volatility_match=False,\n",
    "                     cone_function=forecast_cone_bootstrap,\n",
    "                    ax=axes[0])\n",
    "plot_rolling_sharpe(returns, ax=axes[1], rolling_window=63)\n",
    "axes[0].set_title('Cumulative Returns - In and Out-of-Sample')\n",
    "axes[1].set_title('Rolling Sharpe Ratio (3 Months)')\n",
    "sns.despine()\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T22:09:26.776073Z",
     "start_time": "2020-06-17T22:08:46.855176Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pf.create_full_tear_sheet(returns, \n",
    "                          positions=positions, \n",
    "                          transactions=transactions,\n",
    "                          benchmark_rets=benchmark,\n",
    "                          live_start_date=LIVE_DATE, \n",
    "                          round_trips=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
